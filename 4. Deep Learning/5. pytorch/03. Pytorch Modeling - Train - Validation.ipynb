{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # 데이터를 저장할 root 디렉토리\n",
    "    train=True, # 훈련용 데이터 설정\n",
    "    download=True, # 다운로드\n",
    "    transform=ToTensor() # 이미지 변환. 여기서는 TorchTesnor로 변환시킵니다.\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_data, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_data, batch_size=64, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Modeling\n",
    "파이토치는 대부분 클래스 기반 모델링을 수행합니다. `torch.nn.Module` 클래스를 상속 받아 만들게 됩니다. 필수적으로 오버라이딩 해야 하는 메소드는 생성자 `__init__`과 순전파를 담당하는 `forward` 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    # Subclass인 NeuralNetwork의 생성자.\n",
    "    #   여기에서 상위 클래스인 nn.Module의 생성에 대한 책임을 져야 한다.\n",
    "    #   책임이란? 부모클래스의 생성자에 필요한 파라미터\n",
    "    super(NeuralNetwork, self).__init__()\n",
    "\n",
    "    # 생성자에는 항상 레이어의 구성을 정의\n",
    "    self.flatten = nn.Flatten() # 입력되는 데이터를 평탄화 시키는 레이어\n",
    "\n",
    "    # nn.Sequential을 이용해 연속되는 레이어의 구조를 구성\n",
    "    self.fcl_stack = nn.Sequential(\n",
    "      # 1층 구성\n",
    "      nn.Linear(28*28, 128),\n",
    "      nn.ReLU(),\n",
    "\n",
    "      # 2층 (출력층)\n",
    "      nn.Linear(128, 10)\n",
    "    )\n",
    "    # Softmax를 따로 쓰지 않는 이유는 실제 모델 순전파 시에 넣어도 상관 없기 때문 ! (훈련 할 떄)\n",
    "    # 꼭 없어야 하는건 아님 !! 상황에 따라서 넣어 줄 수도 있음\n",
    "\n",
    "  def forward(self, x):\n",
    "    # forward에는 입력 데이터 x가 들어온다. 이 때 x의 shape은? (N, 1, 28, 28)\n",
    "    x = self.flatten(x) # flatten : 평탄화\n",
    "    y = self.fcl_stack(x)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 생성\n",
    "파이토치를 이용해 모델 객체를 만들고 나서 어떤 장치(device) 환경에서 훈련이나 추론을 수행할지 결정지어줘야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 만들고, 만든 모델을 설정한 환경(device)로 옮긴다는 개념"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fcl_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델을 만들고, 만든 모델을 설정한 환경(device)로 옮긴다는 개념\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 짱짱 중요함\n",
    "\n",
    "# 손실 함수 정의.\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 최적화 기법 정의 (Opitmizer)\n",
    "#   강사하강법을 수행하기 위한 함수. 경사하강법은 어디에 수행하죠? 모델의 W, b에 수행\n",
    "#   W, b는 다른 말로 모델의 파라미터(Model Parameter). model.parameters()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 과정 ( 훈련 루프 정의 )\n",
    "#   1. DataLoader에서 배치 크기 만큼 데이터를 꺼낸다.\n",
    "#   2. 데이터를 모델에 통과시킨다. (순전파를 통한 추론 - prediction(inference))\n",
    "#   3. 순전파(예측)가 끝나면 예측 값이 나오게 되고, 이를 토대로 Loss를 계산\n",
    "#   4. 역전파를 통한 미분값 계산\n",
    "#   5. 얻어낸 미분값으로 경사하강법 수행 ( 최적화 )\n",
    "def train_loop(dataloader, model, loss_fn, optimizer) :\n",
    "    # 데이터 로더에 있는 데이터 세트의 전체 개수 가져오기\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    # 중요!! model을 훈련 모드로 설정. 가중치 갱신이 가능한 상황으로 바꿔주기\n",
    "    model.train()\n",
    "\n",
    "    # 1. 데이터 꺼내기. for문을 사용하면 자동으로 nex(iter(dataloader))가 실행된다.\n",
    "    for batch_idx, (X, y) in enumerate(dataloader) :\n",
    "        # 현재 데이터 로더에 있는 데이터는 cpu에 존재하고 있기 때문에, 이 데이터들은 gpu로 옮겨야 한다.\n",
    "        #   모델이 위치한 곳과, 데이터가 위치한 곳을 동일하게 맞춰준다.\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 2. 순전파 수행\n",
    "        pred = model(X) # 자동으로 forward가 실행된다.\n",
    "\n",
    "        # 3. Loss 계산\n",
    "        loss = loss_fn(pred, y) # 이 때 자동으로 Softmax가 적용된다.\n",
    "\n",
    "        # 4. 역전파 수행 ( 역전파를 수행하면서 각 가중치의 미분값을 얻어낸다.)\n",
    "        optimizer.zero_grad() # 최적화 함수안에 남아있는 기존의 기울기를 제거\n",
    "                              # -> 이전 배치의 기울기가 남아있으면 정확한 기울기를 구하는데 방해가 된다.\n",
    "        loss.backward() # 역전파 수행. Loss가 Leaf\n",
    "\n",
    "        # 5. 최적화 수행(경사하강법 수행)\n",
    "        optimizer.step() # backward()로 구한 미분값을 토대로 최적화\n",
    "\n",
    "        if batch_idx % 100 == 0 :\n",
    "            loss, current = loss.item(), batch_idx * len(X)\n",
    "            print(f\"Train Loss : {loss:>7} [{current:>5d} / {size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론에서는 최적화할 필요가 없다.\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "  size = len(dataloader.dataset)\n",
    "\n",
    "  # loss는 배치 별로 계산, correct는 전체 데이터 세트에 대한 평균 정확도\n",
    "  test_loss, correct = 0, 0\n",
    "\n",
    "  # 모델을 추론 모드로 바꿔준다.\n",
    "  model.eval()\n",
    "\n",
    "  # 추론 과정에서는 기울기를 구할 필요가 없습니다.\n",
    "  #   따라서 모든 파라미터(model.parameters())의 required_grad=False\n",
    "  with torch.no_grad():\n",
    "    # 1. 데이터 꺼내기\n",
    "    for X, y in dataloader:\n",
    "      # 데이터를 gpu로 옮겨주기\n",
    "      #   모델과 같은 장치(device)로 옮겨주기\n",
    "      X, y = X.to(device), y.to(device)\n",
    "\n",
    "      # 2. 순전파 수행\n",
    "      pred = model(X)\n",
    "\n",
    "      # 3. loss 및 accuracy 계산\n",
    "      test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "      # 10개의 예측 값중 가장 큰 곳의 인덱스를 argmax로 찾고, 타겟(y)와 일치하는지 확인\n",
    "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "  # 배치 개수 구하기\n",
    "  num_batches = len(dataloader)\n",
    "\n",
    "  # Loss 평균 구하기\n",
    "  test_loss = test_loss / num_batches\n",
    "\n",
    "  # Accuracy 구하기\n",
    "  correct /= size\n",
    "\n",
    "  print(f\"Test Error : \\n Accuracy : {(100*correct):>0.1f}%, Avg Loss : {test_loss:>8f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      ".....................\n",
      "Train Loss : 2.297231435775757 [    0 / 60000]\n",
      "Train Loss : 1.4752072095870972 [ 6400 / 60000]\n",
      "Train Loss : 1.0246914625167847 [12800 / 60000]\n",
      "Train Loss : 0.9473208785057068 [19200 / 60000]\n",
      "Train Loss : 0.6989604830741882 [25600 / 60000]\n",
      "Train Loss : 0.8133335709571838 [32000 / 60000]\n",
      "Train Loss : 0.7934366464614868 [38400 / 60000]\n",
      "Train Loss : 0.5299373865127563 [44800 / 60000]\n",
      "Train Loss : 0.6296875476837158 [51200 / 60000]\n",
      "Train Loss : 0.5670382976531982 [57600 / 60000]\n",
      "Test Error : \n",
      " Accuracy : 78.8%, Avg Loss : 0.625677\n",
      "\n",
      "Epoch 2\n",
      ".....................\n",
      "Train Loss : 0.7369483113288879 [    0 / 60000]\n",
      "Train Loss : 0.5683687925338745 [ 6400 / 60000]\n",
      "Train Loss : 0.5064662098884583 [12800 / 60000]\n",
      "Train Loss : 0.5606881380081177 [19200 / 60000]\n",
      "Train Loss : 0.5195249319076538 [25600 / 60000]\n",
      "Train Loss : 0.6281054615974426 [32000 / 60000]\n",
      "Train Loss : 0.6380984783172607 [38400 / 60000]\n",
      "Train Loss : 0.6731709241867065 [44800 / 60000]\n",
      "Train Loss : 0.6115968227386475 [51200 / 60000]\n",
      "Train Loss : 0.5381126999855042 [57600 / 60000]\n",
      "Test Error : \n",
      " Accuracy : 81.7%, Avg Loss : 0.538257\n",
      "\n",
      "Epoch 3\n",
      ".....................\n",
      "Train Loss : 0.46187281608581543 [    0 / 60000]\n",
      "Train Loss : 0.5538557767868042 [ 6400 / 60000]\n",
      "Train Loss : 0.6003504395484924 [12800 / 60000]\n",
      "Train Loss : 0.461364209651947 [19200 / 60000]\n",
      "Train Loss : 0.49883097410202026 [25600 / 60000]\n",
      "Train Loss : 0.4281555712223053 [32000 / 60000]\n",
      "Train Loss : 0.5013973712921143 [38400 / 60000]\n",
      "Train Loss : 0.5312983393669128 [44800 / 60000]\n",
      "Train Loss : 0.3568870425224304 [51200 / 60000]\n",
      "Train Loss : 0.46234387159347534 [57600 / 60000]\n",
      "Test Error : \n",
      " Accuracy : 83.0%, Avg Loss : 0.495266\n",
      "\n",
      "Epoch 4\n",
      ".....................\n",
      "Train Loss : 0.5408007502555847 [    0 / 60000]\n",
      "Train Loss : 0.42169344425201416 [ 6400 / 60000]\n",
      "Train Loss : 0.48480668663978577 [12800 / 60000]\n",
      "Train Loss : 0.3841838240623474 [19200 / 60000]\n",
      "Train Loss : 0.36323556303977966 [25600 / 60000]\n",
      "Train Loss : 0.35373368859291077 [32000 / 60000]\n",
      "Train Loss : 0.42500364780426025 [38400 / 60000]\n",
      "Train Loss : 0.42522740364074707 [44800 / 60000]\n",
      "Train Loss : 0.34973642230033875 [51200 / 60000]\n",
      "Train Loss : 0.2783961892127991 [57600 / 60000]\n",
      "Test Error : \n",
      " Accuracy : 83.7%, Avg Loss : 0.472827\n",
      "\n",
      "Epoch 5\n",
      ".....................\n",
      "Train Loss : 0.3794471323490143 [    0 / 60000]\n",
      "Train Loss : 0.2657637298107147 [ 6400 / 60000]\n",
      "Train Loss : 0.5224688649177551 [12800 / 60000]\n",
      "Train Loss : 0.48086291551589966 [19200 / 60000]\n",
      "Train Loss : 0.48057517409324646 [25600 / 60000]\n",
      "Train Loss : 0.5130919218063354 [32000 / 60000]\n",
      "Train Loss : 0.3828074634075165 [38400 / 60000]\n",
      "Train Loss : 0.5066537261009216 [44800 / 60000]\n",
      "Train Loss : 0.31470003724098206 [51200 / 60000]\n",
      "Train Loss : 0.44385263323783875 [57600 / 60000]\n",
      "Test Error : \n",
      " Accuracy : 84.1%, Avg Loss : 0.456054\n",
      "\n",
      "Epoch 6\n",
      ".....................\n",
      "Train Loss : 0.39423900842666626 [    0 / 60000]\n",
      "Train Loss : 0.3215738534927368 [ 6400 / 60000]\n",
      "Train Loss : 0.2710311710834503 [12800 / 60000]\n",
      "Train Loss : 0.42639362812042236 [19200 / 60000]\n",
      "Train Loss : 0.4025150239467621 [25600 / 60000]\n",
      "Train Loss : 0.3601796329021454 [32000 / 60000]\n",
      "Train Loss : 0.3911910951137543 [38400 / 60000]\n",
      "Train Loss : 0.23459169268608093 [44800 / 60000]\n",
      "Train Loss : 0.3348335325717926 [51200 / 60000]\n",
      "Train Loss : 0.3785300552845001 [57600 / 60000]\n",
      "Test Error : \n",
      " Accuracy : 84.5%, Avg Loss : 0.445451\n",
      "\n",
      "Epoch 7\n",
      ".....................\n",
      "Train Loss : 0.28554290533065796 [    0 / 60000]\n",
      "Train Loss : 0.5756221413612366 [ 6400 / 60000]\n",
      "Train Loss : 0.2669033408164978 [12800 / 60000]\n",
      "Train Loss : 0.5002758502960205 [19200 / 60000]\n",
      "Train Loss : 0.45632094144821167 [25600 / 60000]\n",
      "Train Loss : 0.47622916102409363 [32000 / 60000]\n",
      "Train Loss : 0.4932548403739929 [38400 / 60000]\n",
      "Train Loss : 0.37227338552474976 [44800 / 60000]\n",
      "Train Loss : 0.2684416174888611 [51200 / 60000]\n",
      "Train Loss : 0.273860901594162 [57600 / 60000]\n",
      "Test Error : \n",
      " Accuracy : 84.7%, Avg Loss : 0.434678\n",
      "\n",
      "Epoch 8\n",
      ".....................\n",
      "Train Loss : 0.32481515407562256 [    0 / 60000]\n",
      "Train Loss : 0.3922584056854248 [ 6400 / 60000]\n",
      "Train Loss : 0.3589944839477539 [12800 / 60000]\n",
      "Train Loss : 0.3905552327632904 [19200 / 60000]\n",
      "Train Loss : 0.2895108461380005 [25600 / 60000]\n",
      "Train Loss : 0.3208599090576172 [32000 / 60000]\n",
      "Train Loss : 0.4203532934188843 [38400 / 60000]\n",
      "Train Loss : 0.34805697202682495 [44800 / 60000]\n",
      "Train Loss : 0.26028579473495483 [51200 / 60000]\n",
      "Train Loss : 0.4832012355327606 [57600 / 60000]\n",
      "Test Error : \n",
      " Accuracy : 84.9%, Avg Loss : 0.427076\n",
      "\n",
      "Epoch 9\n",
      ".....................\n",
      "Train Loss : 0.37689629197120667 [    0 / 60000]\n",
      "Train Loss : 0.5468209981918335 [ 6400 / 60000]\n",
      "Train Loss : 0.3667951822280884 [12800 / 60000]\n",
      "Train Loss : 0.32459527254104614 [19200 / 60000]\n",
      "Train Loss : 0.28686416149139404 [25600 / 60000]\n",
      "Train Loss : 0.4346489906311035 [32000 / 60000]\n",
      "Train Loss : 0.5112150311470032 [38400 / 60000]\n",
      "Train Loss : 0.31466954946517944 [44800 / 60000]\n",
      "Train Loss : 0.3367321789264679 [51200 / 60000]\n",
      "Train Loss : 0.24940426647663116 [57600 / 60000]\n",
      "Test Error : \n",
      " Accuracy : 85.2%, Avg Loss : 0.419902\n",
      "\n",
      "Epoch 10\n",
      ".....................\n",
      "Train Loss : 0.2908499240875244 [    0 / 60000]\n",
      "Train Loss : 0.3765212893486023 [ 6400 / 60000]\n",
      "Train Loss : 0.5789579749107361 [12800 / 60000]\n",
      "Train Loss : 0.35808122158050537 [19200 / 60000]\n",
      "Train Loss : 0.35596755146980286 [25600 / 60000]\n",
      "Train Loss : 0.37315383553504944 [32000 / 60000]\n",
      "Train Loss : 0.49241235852241516 [38400 / 60000]\n",
      "Train Loss : 0.29743918776512146 [44800 / 60000]\n",
      "Train Loss : 0.19942842423915863 [51200 / 60000]\n",
      "Train Loss : 0.3374313414096832 [57600 / 60000]\n",
      "Test Error : \n",
      " Accuracy : 85.6%, Avg Loss : 0.413722\n",
      "\n",
      "훈련 끝!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for i in range(epochs):\n",
    "  print(f\"Epoch {i+1}\\n.....................\")\n",
    "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "  test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(\"훈련 끝!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련된 모델의 가중치를 저장 / 불러오기\n",
    "불러올 곳에서 **모델의 구조를 알고 있는 경우** 가중치만 저장하면 적은 용량으로 저장하고 불러오는 것이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fcl_stack.0.weight',\n",
       "              tensor([[-0.0292,  0.0073, -0.0116,  ..., -0.0264, -0.0462, -0.0302],\n",
       "                      [-0.0158,  0.0146, -0.0059,  ..., -0.0577, -0.1042, -0.0069],\n",
       "                      [-0.0242, -0.0517, -0.0524,  ..., -0.0679, -0.0490, -0.0611],\n",
       "                      ...,\n",
       "                      [-0.0482, -0.0356, -0.0853,  ..., -0.0545,  0.0269,  0.0081],\n",
       "                      [ 0.0758, -0.0496,  0.0137,  ..., -0.0658, -0.0466,  0.0087],\n",
       "                      [-0.0591, -0.0534, -0.0577,  ..., -0.0074, -0.0091, -0.0518]],\n",
       "                     device='cuda:0')),\n",
       "             ('fcl_stack.0.bias',\n",
       "              tensor([-0.0693,  0.2537, -0.0491,  0.1794,  0.0732,  0.1356,  0.2742, -0.0847,\n",
       "                       0.1632, -0.1957,  0.0778,  0.0637,  0.0085, -0.0930, -0.0127, -0.1241,\n",
       "                       0.1139, -0.0599,  0.0155, -0.0106,  0.1650,  0.0669, -0.0662,  0.1834,\n",
       "                      -0.1016,  0.0265,  0.1381, -0.0177,  0.0821,  0.0029, -0.0186, -0.1966,\n",
       "                      -0.0235,  0.1198,  0.0162,  0.0348,  0.1791, -0.1661, -0.0350,  0.1350,\n",
       "                       0.0097,  0.2022,  0.0390, -0.1237, -0.0110,  0.1508,  0.1021, -0.1202,\n",
       "                      -0.0399,  0.2592, -0.0026,  0.0755,  0.0346, -0.2188,  0.0897,  0.1378,\n",
       "                      -0.2039,  0.0439,  0.2767,  0.0070,  0.2098, -0.0199,  0.0700,  0.0269,\n",
       "                      -0.0440,  0.0719,  0.1391,  0.2256,  0.0289, -0.0129, -0.0076,  0.2366,\n",
       "                       0.0306, -0.0328, -0.0269,  0.1143, -0.0810,  0.0164,  0.0569,  0.1665,\n",
       "                      -0.0495,  0.1638, -0.0291,  0.0607,  0.1277,  0.0672,  0.0017,  0.0576,\n",
       "                       0.0156,  0.0492,  0.1367,  0.1259,  0.0592, -0.1611,  0.1131, -0.1368,\n",
       "                      -0.0978, -0.1806,  0.0336, -0.0159,  0.0596,  0.0165, -0.0487, -0.0594,\n",
       "                       0.0854, -0.0275,  0.2596,  0.1075,  0.1080, -0.0840,  0.1948, -0.0668,\n",
       "                       0.1856,  0.1071,  0.0583,  0.1920,  0.1604,  0.1217,  0.1078,  0.1079,\n",
       "                      -0.1971, -0.1008, -0.1325,  0.0938,  0.2577, -0.2385, -0.1046, -0.1434],\n",
       "                     device='cuda:0')),\n",
       "             ('fcl_stack.2.weight',\n",
       "              tensor([[ 0.0443,  0.0499,  0.0590,  ..., -0.1226,  0.0382, -0.0480],\n",
       "                      [-0.1321, -0.1914, -0.0802,  ...,  0.0672,  0.0270,  0.0770],\n",
       "                      [ 0.0903, -0.0778, -0.0447,  ..., -0.1454, -0.0346,  0.0656],\n",
       "                      ...,\n",
       "                      [ 0.0142,  0.0231, -0.0254,  ...,  0.0328, -0.1530, -0.0955],\n",
       "                      [ 0.1329,  0.0576,  0.1123,  ...,  0.0044,  0.1048,  0.0950],\n",
       "                      [ 0.0560,  0.0033, -0.2705,  ...,  0.1284, -0.0555, -0.2213]],\n",
       "                     device='cuda:0')),\n",
       "             ('fcl_stack.2.bias',\n",
       "              tensor([ 0.0323, -0.0283, -0.0528,  0.0505, -0.1692,  0.2697,  0.1325, -0.0483,\n",
       "                      -0.0022, -0.2517], device='cuda:0'))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델의 구조를 알고 있는 경우( 모델의 클래스를 알고 있는 경우 )\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_params.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fcl_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model2 = NeuralNetwork().to(device) # NeuralNetwork.cuda()\n",
    "print(model2) # model2는 훈련이 되지 않은 새로운 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error : \n",
      " Accuracy : 15.6%, Avg Loss : 2.296766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 훈련되지 않은 모델로 검증하면 당연히 성능이 좋지 않다.\n",
    "test_loop(test_dataloader, model2, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_22608\\856506580.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model2.load_state_dict(torch.load(params_file_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error : \n",
      " Accuracy : 85.6%, Avg Loss : 0.413722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params_file_path = \"model_params.pth\"\n",
    "\n",
    "# 저장된 가중치로 새로운 모델에 덮어쓰기\n",
    "model2.load_state_dict(torch.load(params_file_path))\n",
    "test_loop(test_dataloader, model2, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련된 모델 자체를 저장 / 불러오기\n",
    "모델의 구조를 모르는 경우 사용할 수 있는 대표적인 방법으로서 가중치만 저장한 경우보다 파일의 크기는 크지만 구조를 몰라도 모델을 사용할 수 있다는 장점이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
      "��ġ ������ �ƴմϴ�.\n"
     ]
    }
   ],
   "source": [
    "!ls -al | grep pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error : \n",
      " Accuracy : 85.6%, Avg Loss : 0.413722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# weights_only -> 가중치만 가져오는 옵션\n",
    "model3 = torch.load('model.pth', weights_only=False)\n",
    "test_loop(test_dataloader, model3, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
